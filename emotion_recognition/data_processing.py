"""
æ•°æ®å¤„ç†è„šæœ¬ - ä¸­æ–‡å•æ ‡ç­¾ç‰ˆæœ¬ï¼ˆæ”¹è¿›ç‰ˆï¼‰
åŠ è½½è‡ªå®šä¹‰JSONæ ¼å¼çš„ä¸­æ–‡æƒ…ç»ªæ•°æ®é›†
"""

import json
import torch
import numpy as np
from collections import Counter
from transformers import AutoTokenizer
from torch.utils.data import DataLoader, Dataset
from config import MODEL_CONFIG, TRAINING_CONFIG, EMOJI_TO_ID, EMOJI_LIST, PATH_CONFIG


class EmojiDataset(Dataset):
    """ä¸­æ–‡emojiå•æ ‡ç­¾æ•°æ®é›†ç±»"""
    
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels
    
    def __len__(self):
        return len(self.labels)
    
    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        # å•æ ‡ç­¾åˆ†ç±»ï¼šä½¿ç”¨long tensor
        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)
        return item


def load_json_data(file_path):
    """åŠ è½½JSONæ•°æ®æ–‡ä»¶"""
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return data


def convert_to_single_label(emojis_list):
    """å°†emojiåˆ—è¡¨è½¬æ¢ä¸ºå•æ ‡ç­¾ç´¢å¼•ï¼ˆåªå–ç¬¬ä¸€ä¸ªemojiï¼‰"""
    if emojis_list and emojis_list[0] in EMOJI_TO_ID:
        return EMOJI_TO_ID[emojis_list[0]]
    return 0


def convert_data_to_single_label(data):
    """
    å°†æ•°æ®è½¬æ¢ä¸ºå•æ ‡ç­¾æ ¼å¼ï¼ˆåªå–æ¯æ¡æ•°æ®çš„ç¬¬ä¸€ä¸ªemojiï¼‰
    ä¾‹å¦‚ï¼š{"text": "xxx", "emojis": ["ðŸ˜‚", "ðŸ˜„"]} 
    è½¬æ¢ä¸ºï¼š{"text": "xxx", "label": 0}  (åªä¿ç•™ç¬¬ä¸€ä¸ªemoji ðŸ˜‚)
    """
    converted = []
    for item in data:
        text = item['text']
        emojis = item['emojis']
        if emojis and emojis[0] in EMOJI_TO_ID:
            converted.append({
                'text': text,
                'label': EMOJI_TO_ID[emojis[0]]
            })
    return converted


def compute_class_weights(labels):
    """è®¡ç®—ç±»åˆ«æƒé‡æ¥å¤„ç†ä¸å¹³è¡¡é—®é¢˜"""
    label_counts = Counter(labels)
    total = len(labels)
    num_classes = len(EMOJI_LIST)
    
    weights = []
    for i in range(num_classes):
        count = label_counts.get(i, 1)  # é¿å…é™¤é›¶
        # ä½¿ç”¨ inverse frequency
        weight = total / (num_classes * count)
        weights.append(weight)
    
    # å½’ä¸€åŒ–
    weights = np.array(weights)
    weights = weights / weights.sum() * num_classes
    
    return torch.tensor(weights, dtype=torch.float)


def load_and_process_data():
    """åŠ è½½å¹¶å¤„ç†è‡ªå®šä¹‰ä¸­æ–‡æ•°æ®é›† - å•æ ‡ç­¾ç‰ˆæœ¬ï¼ˆåªå–ç¬¬ä¸€ä¸ªemojiï¼‰"""
    import sys
    
    print("Loading custom Chinese emoji dataset (single-label mode - first emoji only)...")
    sys.stdout.flush()
    
    # åŠ è½½è®­ç»ƒå’ŒéªŒè¯æ•°æ®
    print(f"[DEBUG] Loading train file: {PATH_CONFIG['train_file']}")
    sys.stdout.flush()
    train_data_raw = load_json_data(PATH_CONFIG['train_file'])
    print(f"[DEBUG] Loading val file: {PATH_CONFIG['val_file']}")
    sys.stdout.flush()
    val_data_raw = load_json_data(PATH_CONFIG['val_file'])
    
    # è½¬æ¢ä¸ºå•æ ‡ç­¾ï¼ˆåªå–ç¬¬ä¸€ä¸ªemojiï¼‰
    print("[DEBUG] Converting to single-label (using first emoji only)...")
    sys.stdout.flush()
    train_data = convert_data_to_single_label(train_data_raw)
    val_data = convert_data_to_single_label(val_data_raw)
    
    print(f"Dataset converted (first emoji only):")
    print(f"  Train samples: {len(train_data_raw)} -> {len(train_data)}")
    print(f"  Validation samples: {len(val_data_raw)} -> {len(val_data)}")
    sys.stdout.flush()
    
    # æå–æ–‡æœ¬å’Œæ ‡ç­¾
    train_texts = [item['text'] for item in train_data]
    train_labels = [item['label'] for item in train_data]
    val_texts = [item['text'] for item in val_data]
    val_labels = [item['label'] for item in val_data]
    
    # è®¡ç®—ç±»åˆ«æƒé‡
    print("[DEBUG] Computing class weights...")
    sys.stdout.flush()
    class_weights = compute_class_weights(train_labels)
    print(f"Class weights: {class_weights}")
    sys.stdout.flush()
    
    # åŠ è½½tokenizer
    print(f"\nLoading tokenizer: {MODEL_CONFIG['model_name']}")
    print("[DEBUG] This may take a while if downloading for the first time...")
    sys.stdout.flush()
    tokenizer = AutoTokenizer.from_pretrained(MODEL_CONFIG['model_name'])
    print("[DEBUG] Tokenizer loaded")
    sys.stdout.flush()
    
    # åˆ†è¯
    def tokenize_texts(texts):
        return tokenizer(
            texts,
            padding='max_length',
            truncation=True,
            max_length=MODEL_CONFIG['max_length'],
            return_tensors=None
        )
    
    print("\nTokenizing datasets...")
    sys.stdout.flush()
    tokenized_train = tokenize_texts(train_texts)
    tokenized_val = tokenize_texts(val_texts)
    
    # åˆ›å»º PyTorch Dataset
    train_dataset = EmojiDataset(tokenized_train, train_labels)
    val_dataset = EmojiDataset(tokenized_val, val_labels)
    
    print(f"\nDatasets created:")
    print(f"  Train: {len(train_dataset)} samples")
    print(f"  Validation: {len(val_dataset)} samples")
    sys.stdout.flush()
    
    # ç»Ÿè®¡emojiåˆ†å¸ƒ
    print("\nEmoji distribution in training set:")
    label_counts = Counter(train_labels)
    for i in range(len(EMOJI_LIST)):
        count = label_counts.get(i, 0)
        print(f"  {EMOJI_LIST[i]}: {count}")
    sys.stdout.flush()
    
    return train_dataset, val_dataset, class_weights, tokenizer


def create_dataloaders(train_dataset, val_dataset):
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    
    batch_size = TRAINING_CONFIG['batch_size']
    
    train_loader = DataLoader(
        train_dataset, 
        batch_size=batch_size, 
        shuffle=True,
        num_workers=0,  # é¿å…tokenizerè­¦å‘Š
        pin_memory=True
    )
    
    val_loader = DataLoader(
        val_dataset, 
        batch_size=batch_size, 
        shuffle=False,
        num_workers=0,
        pin_memory=True
    )
    
    return train_loader, val_loader


if __name__ == "__main__":
    # æµ‹è¯•æ•°æ®åŠ è½½
    train_dataset, val_dataset, class_weights, tokenizer = load_and_process_data()
    train_loader, val_loader = create_dataloaders(train_dataset, val_dataset)
    
    # æŸ¥çœ‹ä¸€ä¸ªbatch
    batch = next(iter(train_loader))
    print(f"\nBatch shapes:")
    for key, val in batch.items():
        print(f"  {key}: {val.shape}")
    
    print(f"\nFirst sample label: {batch['labels'][0]} -> {EMOJI_LIST[batch['labels'][0]]}")
